This study introduces a pioneering machine learning (ML)--based methodology for quantifying iron impurities in silicon solar cells.
A comprehensive analysis was done on 80 models, utilizing algorithms such as Random Forest (RF), Gradient Boosting (GB),
eXtreme Gradient Boosting (XGB), Support Vector Regression (SVR), and Deep Neural Networks (DNN)
to predict iron concentration based on variations in photovoltaic parameters caused by FeB pair dissociation.
The conditions the training dataset must meet to minimize forecasting errors were identified,
along with the feature combinations that yield the most accurate predictions.
Furthermore, the effectiveness of using Principal Component Analysis for data pre-processing was assessed.
The results demonstrate that XGB and DNN outperform other models, achieving MSE, MAPE, and R$^2$ values
of up to 0.003, 3\%, and 0.997 for synthetic data and 0.004, 9\%, and 0.987 for experimental data.
