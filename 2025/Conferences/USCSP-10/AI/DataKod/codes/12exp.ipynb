{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f927a6-948b-4f53-80a6-fb65f26c4c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def extract_parameters_from_filename(filename):\n",
    "    iron_concentration = float(filename[3:12])\n",
    "    boron_concentration_str = filename[14:22]\n",
    "    boron_concentration = float(boron_concentration_str.replace('e', 'E'))\n",
    "    temperature = int(filename[23:26])\n",
    "    thickness = float(filename[27:30]) * 1e-6\n",
    "\n",
    "    iron_concentration_log = np.log10(iron_concentration)\n",
    "    return iron_concentration_log\n",
    "\n",
    "def augment_image(img, target_size):\n",
    "    augmented_images = []\n",
    "\n",
    "    left = 11\n",
    "    top = 10\n",
    "    right = 506\n",
    "    bottom = 379\n",
    "    img_cropped = img.crop((left, top, right, bottom))\n",
    "\n",
    "    img_resized = img_cropped.resize(target_size)\n",
    "\n",
    "    augmented_images.append(img_resized)\n",
    "\n",
    "    img_flipped_horizontally = img_resized.transpose(method=Image.FLIP_LEFT_RIGHT)\n",
    "    augmented_images.append(img_flipped_horizontally)\n",
    "\n",
    "    img_flipped_vertically = img_resized.transpose(method=Image.FLIP_TOP_BOTTOM)\n",
    "    augmented_images.append(img_flipped_vertically)\n",
    "\n",
    "    img_rotated_90 = img_resized.rotate(90, expand=True)\n",
    "    img_rotated_180 = img_resized.rotate(180, expand=True)\n",
    "    img_rotated_270 = img_resized.rotate(270, expand=True)\n",
    "\n",
    "    augmented_images.extend([img_rotated_90, img_rotated_180, img_rotated_270])\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "def generate_features(image_directory, target_size, test_directory):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    base_model = EfficientNetB7(weights='imagenet', include_top=True, input_shape=(600, 600, 3))\n",
    "    # base_model = EfficientNetB7(weights='imagenet', include_top=False, pooling='avg', input_shape=(600, 600, 3))\n",
    "\n",
    "    # Load training data from CWT_Images\n",
    "    train_image_filenames = [f for f in os.listdir(image_directory) if f.endswith('.png')]\n",
    "    \n",
    "    for filename in train_image_filenames:\n",
    "        print(f\"Processing (train) file: {filename}\")\n",
    "        try:\n",
    "            iron_concentration_log = extract_parameters_from_filename(filename)\n",
    "            img_path = os.path.join(image_directory, filename)\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            augmented_images = augment_image(img, target_size)\n",
    "\n",
    "            for augmented_img in augmented_images:\n",
    "                img_array = np.array(augmented_img)\n",
    "                img_array_rgb = img_array[:, :, :3]  \n",
    "                img_array_rgb = np.expand_dims(img_array_rgb, axis=0)  \n",
    "\n",
    "                img_array_preprocessed = preprocess_input(img_array_rgb)\n",
    "                predictions = base_model.predict(img_array_preprocessed)\n",
    "                image_features = np.squeeze(predictions)\n",
    "\n",
    "                X_train.append(image_features)\n",
    "                y_train.append(iron_concentration_log)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "    \n",
    "    # Load test data from TEST directory\n",
    "    test_image_filenames = [f for f in os.listdir(test_directory) if f.endswith('.png')]\n",
    "    \n",
    "    for filename in test_image_filenames:\n",
    "        print(f\"Processing (test) file: {filename}\")\n",
    "        try:\n",
    "            iron_concentration_log = extract_parameters_from_filename(filename)\n",
    "            img_path = os.path.join(test_directory, filename)\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            img_cropped = img.crop((11, 10, 506, 379))\n",
    "            img_resized = img_cropped.resize(target_size)\n",
    "            \n",
    "            img_array = np.array(img_resized)\n",
    "            img_array_rgb = img_array[:, :, :3]  \n",
    "            img_array_rgb = np.expand_dims(img_array_rgb, axis=0)  \n",
    "\n",
    "            img_array_preprocessed = preprocess_input(img_array_rgb)\n",
    "            predictions = base_model.predict(img_array_preprocessed)\n",
    "            image_features = np.squeeze(predictions)\n",
    "\n",
    "            X_test.append(image_features)\n",
    "            y_test.append(iron_concentration_log)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    print(f\"Train feature shape: {X_train.shape}, Test feature shape: {X_test.shape}\")\n",
    "    print(f\"Train labels shape: {y_train.shape}, Test labels shape: {y_test.shape}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_regression_model(input_shape):\n",
    "    image_features_input = Input(shape=(input_shape,), name='image_features_input')\n",
    "\n",
    "    x = Dense(128, activation='relu', kernel_regularizer='l2')(image_features_input)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer='l2')(x)\n",
    "    x = Dense(32, activation='relu', kernel_regularizer='l2')(x)\n",
    "\n",
    "    output_layer = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=image_features_input, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "image_directory = 'C:/Users/user/Desktop/wavelets2024/CWT_Images/'\n",
    "test_directory = 'C:/Users/user/Desktop/wavelets2024/TEST/'\n",
    "scaler_directory = 'C:/Users/user/Desktop/wavelets2024/scalers25/'  \n",
    "target_size = (600, 600)\n",
    "\n",
    "X_train, X_test, y_train_log, y_test_log = generate_features(image_directory, target_size, test_directory)\n",
    "\n",
    "if not os.path.exists(scaler_directory):\n",
    "    os.makedirs(scaler_directory)\n",
    "    print(f\"Creating a folder for scalers: {scaler_directory}\")\n",
    "\n",
    "scaler_X_filename = os.path.join(scaler_directory, 'scalerX.bin')\n",
    "scaler_y_filename = os.path.join(scaler_directory, 'scalerY.bin')\n",
    "\n",
    "\n",
    "scalers_loaded = False  # для відслідковування завантаження скалерів\n",
    "\n",
    "if os.path.exists(scaler_X_filename) and os.path.exists(scaler_y_filename):\n",
    "    print(\"Downloading Scalers...\")\n",
    "    scaler_X = joblib.load(scaler_X_filename)\n",
    "    scaler_y = joblib.load(scaler_y_filename)\n",
    "    scalers_loaded = True  # Скалери завантажені\n",
    "else:\n",
    "    print(\"Creating and saving Scalers...\")\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_normalized = scaler_X.fit_transform(X_train)\n",
    "    X_test_normalized = scaler_X.transform(X_test)\n",
    "    \n",
    "    joblib.dump(scaler_X, scaler_X_filename)\n",
    "    \n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_normalized = scaler_y.fit_transform(y_train_log.reshape(-1, 1)).flatten()\n",
    "    y_test_normalized = scaler_y.transform(y_test_log.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    joblib.dump(scaler_y, scaler_y_filename)\n",
    "\n",
    "if scalers_loaded:  # Якщо вони були завантажені\n",
    "    X_train_normalized = scaler_X.transform(X_train)\n",
    "    X_test_normalized = scaler_X.transform(X_test)\n",
    "    \n",
    "    y_train_normalized = scaler_y.transform(y_train_log.reshape(-1, 1)).flatten()\n",
    "    y_test_normalized = scaler_y.transform(y_test_log.reshape(-1, 1)).flatten()\n",
    "\n",
    "# print(f\"X_train mean before standardisation : {np.mean(X_train)}, std before standardisation : {np.std(X_train)}\")\n",
    "# print(f\"y_train mean before standardisation : {np.mean(y_train_log)}, std before standardisation : {np.std(y_train_log)}\")\n",
    "\n",
    "# print(f\"X_train mean after standardisation: {np.mean(X_train_normalized)}, std after standardisation: {np.std(X_train_normalized)}\")\n",
    "# print(f\"y_train mean after standardisation: {np.mean(y_train_normalized)}, std after standardisation: {np.std(y_train_normalized)}\")\n",
    "\n",
    "input_shape = X_train.shape[1]  \n",
    "model = create_regression_model(input_shape) \n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_normalized,\n",
    "    y_train_normalized,\n",
    "    epochs=800,\n",
    "    batch_size=1\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42f753-2ec0-4777-9eb3-3fc1920839e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(X_test_normalized, y_test_normalized)\n",
    "#print(f\"Test Loss: {loss}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "\n",
    "y_pred_log = model.predict(X_test_normalized)\n",
    "\n",
    "# денормалізація\n",
    "y_test_denorm = scaler_y.inverse_transform(y_test_normalized.reshape(-1, 1))\n",
    "y_pred_denorm = scaler_y.inverse_transform(y_pred_log)\n",
    "\n",
    "y_test_exp = np.power(10, y_test_denorm)\n",
    "y_pred_exp = np.power(10, y_pred_denorm)\n",
    "\n",
    "r2_denorm = r2_score(y_test_denorm, y_pred_denorm)\n",
    "\n",
    "r2_exp = r2_score(y_test_exp, y_pred_exp)\n",
    "\n",
    "mape = np.mean(np.abs((y_test_exp - y_pred_exp) / y_test_exp)) * 100  # in percentage\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "with open(\"result_5.dat\", \"w\") as f:\n",
    "    f.write(f\"MSE (normalized): {mse:.8f}\\n\")\n",
    "    f.write(f\"R2 Score (denorm): {r2_denorm:.8f}\\n\")\n",
    "    f.write(f\"R2 Score (10^x): {r2_exp:.8f}\\n\")\n",
    "    f.write(f\"MAPE (%): {mape:.8f}\\n\")\n",
    "\n",
    "# Save the Actual vs. Predicted Iron Concentration to a file\n",
    "true_vs_predict = np.column_stack((y_test_exp, y_pred_exp))\n",
    "np.savetxt(\"TrueVsPredict_5.dat\", true_vs_predict, comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6e0d8-4be8-4bba-b33f-c118b9d34eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test_exp, y_pred_exp, alpha=0.7)\n",
    "plt.plot([min(y_test_exp), max(y_test_exp)], [min(y_test_exp), max(y_test_exp)], 'r--')  # идеальная линия\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Actual Fe concentration\")\n",
    "plt.ylabel(\"Predicted Fe concentration\")\n",
    "plt.title(\"Predicted vs Actual Fe Concentration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbb7f8-44fd-44c1-9b8e-4585b0837cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
